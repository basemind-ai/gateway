// @generated by protobuf-ts 2.9.1 with parameter generate_dependencies,long_type_string,output_javascript_es2020,server_grpc1,force_client_none
// @generated from protobuf file "proto/openai/service/v1/openai.proto" (package "openai.service.v1", syntax proto3)
// tslint:disable
// @generated by protobuf-ts 2.9.1 with parameter generate_dependencies,long_type_string,output_javascript_es2020,server_grpc1,force_client_none
// @generated from protobuf file "proto/openai/service/v1/openai.proto" (package "openai.service.v1", syntax proto3)
// tslint:disable
import { ServiceType } from "@protobuf-ts/runtime-rpc";
import { UnknownFieldHandler } from "@protobuf-ts/runtime";
import { WireType } from "@protobuf-ts/runtime";
import { reflectionMergePartial } from "@protobuf-ts/runtime";
import { MESSAGE_TYPE } from "@protobuf-ts/runtime";
import { MessageType } from "@protobuf-ts/runtime";
/**
 * Type of OpenAI Model
 *
 * @generated from protobuf enum openai.service.v1.OpenAIModel
 */
export var OpenAIModel;
(function (OpenAIModel) {
    /**
     * OpenAI Model is not Specified
     *
     * @generated from protobuf enum value: OPEN_AI_MODEL_UNSPECIFIED = 0;
     */
    OpenAIModel[OpenAIModel["OPEN_AI_MODEL_UNSPECIFIED"] = 0] = "OPEN_AI_MODEL_UNSPECIFIED";
    /**
     * OpenAI GPT3.5 Turbo 4K
     *
     * @generated from protobuf enum value: OPEN_AI_MODEL_GPT3_5_TURBO_4K = 1;
     */
    OpenAIModel[OpenAIModel["OPEN_AI_MODEL_GPT3_5_TURBO_4K"] = 1] = "OPEN_AI_MODEL_GPT3_5_TURBO_4K";
    /**
     * OpenAI GPT3.5 Turbo 16K
     *
     * @generated from protobuf enum value: OPEN_AI_MODEL_GPT3_5_TURBO_16K = 2;
     */
    OpenAIModel[OpenAIModel["OPEN_AI_MODEL_GPT3_5_TURBO_16K"] = 2] = "OPEN_AI_MODEL_GPT3_5_TURBO_16K";
    /**
     * OpenAI GPT4 8K
     *
     * @generated from protobuf enum value: OPEN_AI_MODEL_GPT4_8K = 3;
     */
    OpenAIModel[OpenAIModel["OPEN_AI_MODEL_GPT4_8K"] = 3] = "OPEN_AI_MODEL_GPT4_8K";
    /**
     * OpenAI GPT4 32K
     *
     * @generated from protobuf enum value: OPEN_AI_MODEL_GPT4_32K = 4;
     */
    OpenAIModel[OpenAIModel["OPEN_AI_MODEL_GPT4_32K"] = 4] = "OPEN_AI_MODEL_GPT4_32K";
})(OpenAIModel || (OpenAIModel = {}));
/**
 * Type of OpenAI Message
 *
 * @generated from protobuf enum openai.service.v1.OpenAIMessage
 */
export var OpenAIMessage;
(function (OpenAIMessage) {
    /**
     * OpenAI Message type is not Specified
     *
     * @generated from protobuf enum value: OPEN_AI_MESSAGE_UNSPECIFIED = 0;
     */
    OpenAIMessage[OpenAIMessage["OPEN_AI_MESSAGE_UNSPECIFIED"] = 0] = "OPEN_AI_MESSAGE_UNSPECIFIED";
    /**
     * OpenAI System message
     *
     * @generated from protobuf enum value: OPEN_AI_MESSAGE_SYSTEM = 1;
     */
    OpenAIMessage[OpenAIMessage["OPEN_AI_MESSAGE_SYSTEM"] = 1] = "OPEN_AI_MESSAGE_SYSTEM";
    /**
     * OpenAI User message
     *
     * @generated from protobuf enum value: OPEN_AI_MESSAGE_USER = 2;
     */
    OpenAIMessage[OpenAIMessage["OPEN_AI_MESSAGE_USER"] = 2] = "OPEN_AI_MESSAGE_USER";
    /**
     * OpenAI Assistant message
     *
     * @generated from protobuf enum value: OPEN_AI_MESSAGE_ASSISTANT = 3;
     */
    OpenAIMessage[OpenAIMessage["OPEN_AI_MESSAGE_ASSISTANT"] = 3] = "OPEN_AI_MESSAGE_ASSISTANT";
    /**
     * OpenAI Function message
     *
     * @generated from protobuf enum value: OPEN_AI_MESSAGE_FUNCTION = 4;
     */
    OpenAIMessage[OpenAIMessage["OPEN_AI_MESSAGE_FUNCTION"] = 4] = "OPEN_AI_MESSAGE_FUNCTION";
})(OpenAIMessage || (OpenAIMessage = {}));
// @generated message type with reflection information, may provide speed optimized methods
class OpenAIPromptRequest$Type extends MessageType {
    constructor() {
        super("openai.service.v1.OpenAIPromptRequest", [
            { no: 1, name: "model", kind: "enum", T: () => ["openai.service.v1.OpenAIModel", OpenAIModel] },
            { no: 2, name: "content", kind: "enum", repeat: 1 /*RepeatType.PACKED*/, T: () => ["openai.service.v1.OpenAIMessage", OpenAIMessage] },
            { no: 3, name: "temperature", kind: "scalar", opt: true, T: 2 /*ScalarType.FLOAT*/ },
            { no: 4, name: "top_p", kind: "scalar", opt: true, T: 2 /*ScalarType.FLOAT*/ },
            { no: 5, name: "max_tokens", kind: "scalar", opt: true, T: 13 /*ScalarType.UINT32*/ },
            { no: 6, name: "user_id", kind: "scalar", opt: true, T: 9 /*ScalarType.STRING*/ },
            { no: 7, name: "presence_penalty", kind: "scalar", opt: true, T: 2 /*ScalarType.FLOAT*/ },
            { no: 8, name: "frequency_penalty", kind: "scalar", opt: true, T: 2 /*ScalarType.FLOAT*/ }
        ]);
    }
    create(value) {
        const message = { model: 0, content: [] };
        globalThis.Object.defineProperty(message, MESSAGE_TYPE, { enumerable: false, value: this });
        if (value !== undefined)
            reflectionMergePartial(this, message, value);
        return message;
    }
    internalBinaryRead(reader, length, options, target) {
        let message = target ?? this.create(), end = reader.pos + length;
        while (reader.pos < end) {
            let [fieldNo, wireType] = reader.tag();
            switch (fieldNo) {
                case /* openai.service.v1.OpenAIModel model */ 1:
                    message.model = reader.int32();
                    break;
                case /* repeated openai.service.v1.OpenAIMessage content = 2 [packed = true];*/ 2:
                    if (wireType === WireType.LengthDelimited)
                        for (let e = reader.int32() + reader.pos; reader.pos < e;)
                            message.content.push(reader.int32());
                    else
                        message.content.push(reader.int32());
                    break;
                case /* optional float temperature */ 3:
                    message.temperature = reader.float();
                    break;
                case /* optional float top_p */ 4:
                    message.topP = reader.float();
                    break;
                case /* optional uint32 max_tokens */ 5:
                    message.maxTokens = reader.uint32();
                    break;
                case /* optional string user_id */ 6:
                    message.userId = reader.string();
                    break;
                case /* optional float presence_penalty */ 7:
                    message.presencePenalty = reader.float();
                    break;
                case /* optional float frequency_penalty */ 8:
                    message.frequencyPenalty = reader.float();
                    break;
                default:
                    let u = options.readUnknownField;
                    if (u === "throw")
                        throw new globalThis.Error(`Unknown field ${fieldNo} (wire type ${wireType}) for ${this.typeName}`);
                    let d = reader.skip(wireType);
                    if (u !== false)
                        (u === true ? UnknownFieldHandler.onRead : u)(this.typeName, message, fieldNo, wireType, d);
            }
        }
        return message;
    }
    internalBinaryWrite(message, writer, options) {
        /* openai.service.v1.OpenAIModel model = 1; */
        if (message.model !== 0)
            writer.tag(1, WireType.Varint).int32(message.model);
        /* repeated openai.service.v1.OpenAIMessage content = 2 [packed = true]; */
        if (message.content.length) {
            writer.tag(2, WireType.LengthDelimited).fork();
            for (let i = 0; i < message.content.length; i++)
                writer.int32(message.content[i]);
            writer.join();
        }
        /* optional float temperature = 3; */
        if (message.temperature !== undefined)
            writer.tag(3, WireType.Bit32).float(message.temperature);
        /* optional float top_p = 4; */
        if (message.topP !== undefined)
            writer.tag(4, WireType.Bit32).float(message.topP);
        /* optional uint32 max_tokens = 5; */
        if (message.maxTokens !== undefined)
            writer.tag(5, WireType.Varint).uint32(message.maxTokens);
        /* optional string user_id = 6; */
        if (message.userId !== undefined)
            writer.tag(6, WireType.LengthDelimited).string(message.userId);
        /* optional float presence_penalty = 7; */
        if (message.presencePenalty !== undefined)
            writer.tag(7, WireType.Bit32).float(message.presencePenalty);
        /* optional float frequency_penalty = 8; */
        if (message.frequencyPenalty !== undefined)
            writer.tag(8, WireType.Bit32).float(message.frequencyPenalty);
        let u = options.writeUnknownFields;
        if (u !== false)
            (u == true ? UnknownFieldHandler.onWrite : u)(this.typeName, message, writer);
        return writer;
    }
}
/**
 * @generated MessageType for protobuf message openai.service.v1.OpenAIPromptRequest
 */
export const OpenAIPromptRequest = new OpenAIPromptRequest$Type();
// @generated message type with reflection information, may provide speed optimized methods
class PromptResponse$Type extends MessageType {
    constructor() {
        super("openai.service.v1.PromptResponse", [
            { no: 1, name: "content", kind: "scalar", T: 9 /*ScalarType.STRING*/ },
            { no: 2, name: "prompt_tokens", kind: "scalar", T: 13 /*ScalarType.UINT32*/ },
            { no: 3, name: "completion_tokens", kind: "scalar", T: 13 /*ScalarType.UINT32*/ },
            { no: 4, name: "total_tokens", kind: "scalar", T: 13 /*ScalarType.UINT32*/ }
        ]);
    }
    create(value) {
        const message = { content: "", promptTokens: 0, completionTokens: 0, totalTokens: 0 };
        globalThis.Object.defineProperty(message, MESSAGE_TYPE, { enumerable: false, value: this });
        if (value !== undefined)
            reflectionMergePartial(this, message, value);
        return message;
    }
    internalBinaryRead(reader, length, options, target) {
        let message = target ?? this.create(), end = reader.pos + length;
        while (reader.pos < end) {
            let [fieldNo, wireType] = reader.tag();
            switch (fieldNo) {
                case /* string content */ 1:
                    message.content = reader.string();
                    break;
                case /* uint32 prompt_tokens */ 2:
                    message.promptTokens = reader.uint32();
                    break;
                case /* uint32 completion_tokens */ 3:
                    message.completionTokens = reader.uint32();
                    break;
                case /* uint32 total_tokens */ 4:
                    message.totalTokens = reader.uint32();
                    break;
                default:
                    let u = options.readUnknownField;
                    if (u === "throw")
                        throw new globalThis.Error(`Unknown field ${fieldNo} (wire type ${wireType}) for ${this.typeName}`);
                    let d = reader.skip(wireType);
                    if (u !== false)
                        (u === true ? UnknownFieldHandler.onRead : u)(this.typeName, message, fieldNo, wireType, d);
            }
        }
        return message;
    }
    internalBinaryWrite(message, writer, options) {
        /* string content = 1; */
        if (message.content !== "")
            writer.tag(1, WireType.LengthDelimited).string(message.content);
        /* uint32 prompt_tokens = 2; */
        if (message.promptTokens !== 0)
            writer.tag(2, WireType.Varint).uint32(message.promptTokens);
        /* uint32 completion_tokens = 3; */
        if (message.completionTokens !== 0)
            writer.tag(3, WireType.Varint).uint32(message.completionTokens);
        /* uint32 total_tokens = 4; */
        if (message.totalTokens !== 0)
            writer.tag(4, WireType.Varint).uint32(message.totalTokens);
        let u = options.writeUnknownFields;
        if (u !== false)
            (u == true ? UnknownFieldHandler.onWrite : u)(this.typeName, message, writer);
        return writer;
    }
}
/**
 * @generated MessageType for protobuf message openai.service.v1.PromptResponse
 */
export const PromptResponse = new PromptResponse$Type();
// @generated message type with reflection information, may provide speed optimized methods
class StreamResponse$Type extends MessageType {
    constructor() {
        super("openai.service.v1.StreamResponse", [
            { no: 1, name: "content", kind: "scalar", T: 9 /*ScalarType.STRING*/ },
            { no: 2, name: "finish_reason", kind: "scalar", opt: true, T: 9 /*ScalarType.STRING*/ }
        ]);
    }
    create(value) {
        const message = { content: "" };
        globalThis.Object.defineProperty(message, MESSAGE_TYPE, { enumerable: false, value: this });
        if (value !== undefined)
            reflectionMergePartial(this, message, value);
        return message;
    }
    internalBinaryRead(reader, length, options, target) {
        let message = target ?? this.create(), end = reader.pos + length;
        while (reader.pos < end) {
            let [fieldNo, wireType] = reader.tag();
            switch (fieldNo) {
                case /* string content */ 1:
                    message.content = reader.string();
                    break;
                case /* optional string finish_reason */ 2:
                    message.finishReason = reader.string();
                    break;
                default:
                    let u = options.readUnknownField;
                    if (u === "throw")
                        throw new globalThis.Error(`Unknown field ${fieldNo} (wire type ${wireType}) for ${this.typeName}`);
                    let d = reader.skip(wireType);
                    if (u !== false)
                        (u === true ? UnknownFieldHandler.onRead : u)(this.typeName, message, fieldNo, wireType, d);
            }
        }
        return message;
    }
    internalBinaryWrite(message, writer, options) {
        /* string content = 1; */
        if (message.content !== "")
            writer.tag(1, WireType.LengthDelimited).string(message.content);
        /* optional string finish_reason = 2; */
        if (message.finishReason !== undefined)
            writer.tag(2, WireType.LengthDelimited).string(message.finishReason);
        let u = options.writeUnknownFields;
        if (u !== false)
            (u == true ? UnknownFieldHandler.onWrite : u)(this.typeName, message, writer);
        return writer;
    }
}
/**
 * @generated MessageType for protobuf message openai.service.v1.StreamResponse
 */
export const StreamResponse = new StreamResponse$Type();
/**
 * @generated ServiceType for protobuf service openai.service.v1.OpenAIService
 */
export const OpenAIService = new ServiceType("openai.service.v1.OpenAIService", [
    { name: "OpenAIPrompt", options: {}, I: OpenAIPromptRequest, O: PromptResponse },
    { name: "OpenAIStream", serverStreaming: true, options: {}, I: OpenAIPromptRequest, O: PromptResponse }
]);
