// @generated by protobuf-ts 2.9.1 with parameter generate_dependencies,long_type_string,output_javascript_es2020,server_grpc1,force_client_none
// @generated from protobuf file "proto/openai/service/v1/openai.proto" (package "openai.service.v1", syntax proto3)
// tslint:disable
import type { BinaryWriteOptions } from "@protobuf-ts/runtime";
import type { IBinaryWriter } from "@protobuf-ts/runtime";
import type { BinaryReadOptions } from "@protobuf-ts/runtime";
import type { IBinaryReader } from "@protobuf-ts/runtime";
import type { PartialMessage } from "@protobuf-ts/runtime";
import { MessageType } from "@protobuf-ts/runtime";
/**
 * A Request for an OpenAI regular LLM Prompt
 *
 * @generated from protobuf message openai.service.v1.OpenAIPromptRequest
 */
export interface OpenAIPromptRequest {
    /**
     * OpenAI Model identifier
     *
     * @generated from protobuf field: openai.service.v1.OpenAIModel model = 1;
     */
    model: OpenAIModel;
    /**
     * Prompt Content
     *
     * @generated from protobuf field: repeated openai.service.v1.OpenAIMessage content = 2 [packed = true];
     */
    content: OpenAIMessage[];
    /**
     * Temperature Sampling: https://platform.openai.com/docs/api-reference/chat/create#temperature
     *
     * @generated from protobuf field: optional float temperature = 3;
     */
    temperature?: number;
    /**
     * Nucleus Sampling: https://platform.openai.com/docs/api-reference/chat/create#top_p
     *
     * @generated from protobuf field: optional float top_p = 4;
     */
    topP?: number;
    /**
     * Maximum Tokens after which the prompt will stop generating a response;
     *
     * @generated from protobuf field: optional uint32 max_tokens = 5;
     */
    maxTokens?: number;
    /**
     * Unique user ID to keep track of conversations;
     *
     * @generated from protobuf field: optional string user_id = 6;
     */
    userId?: string;
    /**
     * Penalize New tokens: https://platform.openai.com/docs/api-reference/chat/create#presence_penalty
     *
     * @generated from protobuf field: optional float presence_penalty = 7;
     */
    presencePenalty?: number;
    /**
     * Penalize Repeated tokens: https://platform.openai.com/docs/api-reference/chat/create#frequency_penalty
     *
     * @generated from protobuf field: optional float frequency_penalty = 8;
     */
    frequencyPenalty?: number;
}
/**
 * An OpenAI Prompt Response Message
 *
 * @generated from protobuf message openai.service.v1.PromptResponse
 */
export interface PromptResponse {
    /**
     * Prompt Content
     *
     * @generated from protobuf field: string content = 1;
     */
    content: string;
    /**
     * Number of tokens used for the prompt
     *
     * @generated from protobuf field: uint32 prompt_tokens = 2;
     */
    promptTokens: number;
    /**
     * Number of tokens used to generate the completion
     *
     * @generated from protobuf field: uint32 completion_tokens = 3;
     */
    completionTokens: number;
    /**
     * Total number of tokens used to generate the response
     *
     * @generated from protobuf field: uint32 total_tokens = 4;
     */
    totalTokens: number;
}
/**
 * An OpenAI Streaming Response Message
 *
 * @generated from protobuf message openai.service.v1.StreamResponse
 */
export interface StreamResponse {
    /**
     * Prompt Content
     *
     * @generated from protobuf field: string content = 1;
     */
    content: string;
    /**
     * Finish reason, if this is the last message
     *
     * @generated from protobuf field: optional string finish_reason = 2;
     */
    finishReason?: string;
}
/**
 * Type of OpenAI Model
 *
 * @generated from protobuf enum openai.service.v1.OpenAIModel
 */
export declare enum OpenAIModel {
    /**
     * OpenAI Model is not Specified
     *
     * @generated from protobuf enum value: OPEN_AI_MODEL_UNSPECIFIED = 0;
     */
    OPEN_AI_MODEL_UNSPECIFIED = 0,
    /**
     * OpenAI GPT3.5 Turbo 4K
     *
     * @generated from protobuf enum value: OPEN_AI_MODEL_GPT3_5_TURBO_4K = 1;
     */
    OPEN_AI_MODEL_GPT3_5_TURBO_4K = 1,
    /**
     * OpenAI GPT3.5 Turbo 16K
     *
     * @generated from protobuf enum value: OPEN_AI_MODEL_GPT3_5_TURBO_16K = 2;
     */
    OPEN_AI_MODEL_GPT3_5_TURBO_16K = 2,
    /**
     * OpenAI GPT4 8K
     *
     * @generated from protobuf enum value: OPEN_AI_MODEL_GPT4_8K = 3;
     */
    OPEN_AI_MODEL_GPT4_8K = 3,
    /**
     * OpenAI GPT4 32K
     *
     * @generated from protobuf enum value: OPEN_AI_MODEL_GPT4_32K = 4;
     */
    OPEN_AI_MODEL_GPT4_32K = 4
}
/**
 * Type of OpenAI Message
 *
 * @generated from protobuf enum openai.service.v1.OpenAIMessage
 */
export declare enum OpenAIMessage {
    /**
     * OpenAI Message type is not Specified
     *
     * @generated from protobuf enum value: OPEN_AI_MESSAGE_UNSPECIFIED = 0;
     */
    OPEN_AI_MESSAGE_UNSPECIFIED = 0,
    /**
     * OpenAI System message
     *
     * @generated from protobuf enum value: OPEN_AI_MESSAGE_SYSTEM = 1;
     */
    OPEN_AI_MESSAGE_SYSTEM = 1,
    /**
     * OpenAI User message
     *
     * @generated from protobuf enum value: OPEN_AI_MESSAGE_USER = 2;
     */
    OPEN_AI_MESSAGE_USER = 2,
    /**
     * OpenAI Assistant message
     *
     * @generated from protobuf enum value: OPEN_AI_MESSAGE_ASSISTANT = 3;
     */
    OPEN_AI_MESSAGE_ASSISTANT = 3,
    /**
     * OpenAI Function message
     *
     * @generated from protobuf enum value: OPEN_AI_MESSAGE_FUNCTION = 4;
     */
    OPEN_AI_MESSAGE_FUNCTION = 4
}
declare class OpenAIPromptRequest$Type extends MessageType<OpenAIPromptRequest> {
    constructor();
    create(value?: PartialMessage<OpenAIPromptRequest>): OpenAIPromptRequest;
    internalBinaryRead(reader: IBinaryReader, length: number, options: BinaryReadOptions, target?: OpenAIPromptRequest): OpenAIPromptRequest;
    internalBinaryWrite(message: OpenAIPromptRequest, writer: IBinaryWriter, options: BinaryWriteOptions): IBinaryWriter;
}
/**
 * @generated MessageType for protobuf message openai.service.v1.OpenAIPromptRequest
 */
export declare const OpenAIPromptRequest: OpenAIPromptRequest$Type;
declare class PromptResponse$Type extends MessageType<PromptResponse> {
    constructor();
    create(value?: PartialMessage<PromptResponse>): PromptResponse;
    internalBinaryRead(reader: IBinaryReader, length: number, options: BinaryReadOptions, target?: PromptResponse): PromptResponse;
    internalBinaryWrite(message: PromptResponse, writer: IBinaryWriter, options: BinaryWriteOptions): IBinaryWriter;
}
/**
 * @generated MessageType for protobuf message openai.service.v1.PromptResponse
 */
export declare const PromptResponse: PromptResponse$Type;
declare class StreamResponse$Type extends MessageType<StreamResponse> {
    constructor();
    create(value?: PartialMessage<StreamResponse>): StreamResponse;
    internalBinaryRead(reader: IBinaryReader, length: number, options: BinaryReadOptions, target?: StreamResponse): StreamResponse;
    internalBinaryWrite(message: StreamResponse, writer: IBinaryWriter, options: BinaryWriteOptions): IBinaryWriter;
}
/**
 * @generated MessageType for protobuf message openai.service.v1.StreamResponse
 */
export declare const StreamResponse: StreamResponse$Type;
/**
 * @generated ServiceType for protobuf service openai.service.v1.OpenAIService
 */
export declare const OpenAIService: any;
export {};
