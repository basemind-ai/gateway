// @generated by protobuf-ts 2.9.3 with parameter generate_dependencies,long_type_string,output_javascript_es2020,server_grpc1,force_client_none
// @generated from protobuf file "openai/v1/openai.proto" (package "openai.v1", syntax proto3)
// tslint:disable
import { MessageType } from "@protobuf-ts/runtime";
/**
 * An OpenAI function call
 *
 * @generated from protobuf message openai.v1.OpenAIFunctionCall
 */
export interface OpenAIFunctionCall {
    /**
     * The signature of the function arguments
     *
     * @generated from protobuf field: string arguments = 1;
     */
    arguments: string;
    /**
     * The function name
     *
     * @generated from protobuf field: string name = 2;
     */
    name: string;
}
/**
 * An OpenAI Chat Message
 *
 * @generated from protobuf message openai.v1.OpenAIMessage
 */
export interface OpenAIMessage {
    /**
     * The content of the message
     *
     * @generated from protobuf field: optional string content = 1;
     */
    content?: string;
    /**
     * The role of the message
     *
     * @generated from protobuf field: openai.v1.OpenAIMessageRole role = 2;
     */
    role: OpenAIMessageRole;
    /**
     * Name of the message author or function name
     *
     * @generated from protobuf field: optional string name = 3;
     */
    name?: string;
    /**
     * The signature function to invoke, if any
     *
     * @generated from protobuf field: optional openai.v1.OpenAIFunctionCall function_call = 4;
     */
    functionCall?: OpenAIFunctionCall;
}
/**
 * OpenAI API Request parameters
 *
 * @generated from protobuf message openai.v1.OpenAIModelParameters
 */
export interface OpenAIModelParameters {
    /**
     * Temperature Sampling: https://platform.openai.com/docs/api-reference/chat/create#temperature
     *
     * @generated from protobuf field: optional float temperature = 1;
     */
    temperature?: number;
    /**
     * Nucleus Sampling: https://platform.openai.com/docs/api-reference/chat/create#top_p
     *
     * @generated from protobuf field: optional float top_p = 2;
     */
    topP?: number;
    /**
     * Maximum Tokens after which the prompt will stop generating a response;
     *
     * @generated from protobuf field: optional uint32 max_tokens = 3;
     */
    maxTokens?: number;
    /**
     * Penalize New tokens: https://platform.openai.com/docs/api-reference/chat/create#presence_penalty
     *
     * @generated from protobuf field: optional float presence_penalty = 4;
     */
    presencePenalty?: number;
    /**
     * Penalize Repeated tokens: https://platform.openai.com/docs/api-reference/chat/create#frequency_penalty
     *
     * @generated from protobuf field: optional float frequency_penalty = 5;
     */
    frequencyPenalty?: number;
}
/**
 * A Request for an OpenAI regular LLM Prompt
 *
 * @generated from protobuf message openai.v1.OpenAIPromptRequest
 */
export interface OpenAIPromptRequest {
    /**
     * OpenAI Model identifier
     *
     * @generated from protobuf field: openai.v1.OpenAIModel model = 1;
     */
    model: OpenAIModel;
    /**
     * Prompt Messages
     *
     * @generated from protobuf field: repeated openai.v1.OpenAIMessage messages = 2;
     */
    messages: OpenAIMessage[];
    /**
     * OpenAI API Request parameters
     *
     * @generated from protobuf field: openai.v1.OpenAIModelParameters parameters = 3;
     */
    parameters?: OpenAIModelParameters;
    /**
     * Unique application ID to keep track of conversations;
     *
     * @generated from protobuf field: optional string application_id = 4;
     */
    applicationId?: string;
}
/**
 * An OpenAI Prompt Response Message
 *
 * @generated from protobuf message openai.v1.OpenAIPromptResponse
 */
export interface OpenAIPromptResponse {
    /**
     * Prompt Content
     *
     * @generated from protobuf field: string content = 1;
     */
    content: string;
    /**
     * Count of the request tokens, as returned by the Cohere /tokenize endpoint
     *
     * @generated from protobuf field: uint32 request_tokens_count = 2;
     */
    requestTokensCount: number;
    /**
     * Count of the response tokens, as returned by the Cohere /tokenize endpoint
     *
     * @generated from protobuf field: uint32 response_tokens_count = 3;
     */
    responseTokensCount: number;
    /**
     * Finish reason
     *
     * @generated from protobuf field: string finish_reason = 4;
     */
    finishReason: string;
}
/**
 * An OpenAI Streaming Response Message
 *
 * @generated from protobuf message openai.v1.OpenAIStreamResponse
 */
export interface OpenAIStreamResponse {
    /**
     * Prompt Content
     *
     * @generated from protobuf field: string content = 1;
     */
    content: string;
    /**
     * Finish reason, if this is the last message
     *
     * @generated from protobuf field: optional string finish_reason = 2;
     */
    finishReason?: string;
    /**
     * Count of the request tokens, as returned by the Cohere /tokenize endpoint
     *
     * @generated from protobuf field: optional uint32 request_tokens_count = 3;
     */
    requestTokensCount?: number;
    /**
     * Count of the response tokens, as returned by the Cohere /tokenize endpoint
     *
     * @generated from protobuf field: optional uint32 response_tokens_count = 4;
     */
    responseTokensCount?: number;
}
/**
 * Type of OpenAI Model
 *
 * @generated from protobuf enum openai.v1.OpenAIModel
 */
export declare enum OpenAIModel {
    /**
     * OpenAI Model is not Specified
     *
     * @generated from protobuf enum value: OPEN_AI_MODEL_UNSPECIFIED = 0;
     */
    OPEN_AI_MODEL_UNSPECIFIED = 0,
    /**
     * OpenAI GPT3.5 Turbo 4K
     *
     * @generated from protobuf enum value: OPEN_AI_MODEL_GPT3_5_TURBO_4K = 1;
     */
    OPEN_AI_MODEL_GPT3_5_TURBO_4K = 1,
    /**
     * OpenAI GPT3.5 Turbo 16K
     *
     * @generated from protobuf enum value: OPEN_AI_MODEL_GPT3_5_TURBO_16K = 2;
     */
    OPEN_AI_MODEL_GPT3_5_TURBO_16K = 2,
    /**
     * OpenAI GPT4 8K
     *
     * @generated from protobuf enum value: OPEN_AI_MODEL_GPT4_8K = 3;
     */
    OPEN_AI_MODEL_GPT4_8K = 3,
    /**
     * OpenAI GPT4 32K
     *
     * @generated from protobuf enum value: OPEN_AI_MODEL_GPT4_32K = 4;
     */
    OPEN_AI_MODEL_GPT4_32K = 4
}
/**
 * Type of OpenAI Message
 *
 * @generated from protobuf enum openai.v1.OpenAIMessageRole
 */
export declare enum OpenAIMessageRole {
    /**
     * OpenAI Message type is not Specified
     *
     * @generated from protobuf enum value: OPEN_AI_MESSAGE_ROLE_UNSPECIFIED = 0;
     */
    OPEN_AI_MESSAGE_ROLE_UNSPECIFIED = 0,
    /**
     * OpenAI System message
     *
     * @generated from protobuf enum value: OPEN_AI_MESSAGE_ROLE_SYSTEM = 1;
     */
    OPEN_AI_MESSAGE_ROLE_SYSTEM = 1,
    /**
     * OpenAI User message
     *
     * @generated from protobuf enum value: OPEN_AI_MESSAGE_ROLE_USER = 2;
     */
    OPEN_AI_MESSAGE_ROLE_USER = 2,
    /**
     * OpenAI Assistant message
     *
     * @generated from protobuf enum value: OPEN_AI_MESSAGE_ROLE_ASSISTANT = 3;
     */
    OPEN_AI_MESSAGE_ROLE_ASSISTANT = 3,
    /**
     * OpenAI Function message
     *
     * @generated from protobuf enum value: OPEN_AI_MESSAGE_ROLE_FUNCTION = 4;
     */
    OPEN_AI_MESSAGE_ROLE_FUNCTION = 4
}
declare class OpenAIFunctionCall$Type extends MessageType<OpenAIFunctionCall> {
    constructor();
}
/**
 * @generated MessageType for protobuf message openai.v1.OpenAIFunctionCall
 */
export declare const OpenAIFunctionCall: OpenAIFunctionCall$Type;
declare class OpenAIMessage$Type extends MessageType<OpenAIMessage> {
    constructor();
}
/**
 * @generated MessageType for protobuf message openai.v1.OpenAIMessage
 */
export declare const OpenAIMessage: OpenAIMessage$Type;
declare class OpenAIModelParameters$Type extends MessageType<OpenAIModelParameters> {
    constructor();
}
/**
 * @generated MessageType for protobuf message openai.v1.OpenAIModelParameters
 */
export declare const OpenAIModelParameters: OpenAIModelParameters$Type;
declare class OpenAIPromptRequest$Type extends MessageType<OpenAIPromptRequest> {
    constructor();
}
/**
 * @generated MessageType for protobuf message openai.v1.OpenAIPromptRequest
 */
export declare const OpenAIPromptRequest: OpenAIPromptRequest$Type;
declare class OpenAIPromptResponse$Type extends MessageType<OpenAIPromptResponse> {
    constructor();
}
/**
 * @generated MessageType for protobuf message openai.v1.OpenAIPromptResponse
 */
export declare const OpenAIPromptResponse: OpenAIPromptResponse$Type;
declare class OpenAIStreamResponse$Type extends MessageType<OpenAIStreamResponse> {
    constructor();
}
/**
 * @generated MessageType for protobuf message openai.v1.OpenAIStreamResponse
 */
export declare const OpenAIStreamResponse: OpenAIStreamResponse$Type;
/**
 * @generated ServiceType for protobuf service openai.v1.OpenAIService
 */
export declare const OpenAIService: any;
export {};
