syntax = "proto3";

package prompt_testing.v1;

option go_package = "github.com/basemind-ai/monorepo/gen/prompttesting";

// The Prompt Testing service definition.
service PromptTestingService {
  // Request a streaming LLM prompt
  rpc TestPrompt(PromptTestRequest) returns (stream PromptTestingStreamingPromptResponse) {}
}

// A request for a prompt - sending user input to the server.
message PromptTestRequest {
  // The application ID
  string application_id = 1;
  // The project ID
  optional string prompt_config_id = 2;
  // The model vendor, for example "OPEN_AI"
  string model_vendor = 3;
  // The model type to use, for example "gpt-3.5-turbo"
  string model_type = 4;
  // A serialized JSON object containing the model parameters
  bytes model_parameters = 5;
  // A serialized JSON array of provider message objects
  bytes provider_prompt_messages = 6;
  // The User prompt variables
  // This is a hash-map of variables that should have the same keys as those contained by the PromptConfigResponse
  bytes template_variables = 7;
}

// An Streaming Prompt Response Message
message PromptTestingStreamingPromptResponse {
  // Prompt Content
  string content = 1;
  // Finish reason, given when the stream ends
  optional string finish_reason = 2;
  // Number of tokens used for the prompt request, given when the stream ends
  optional uint32 request_tokens = 3;
  // Number of tokens used for the prompt response, given when the stream ends
  optional uint32 response_tokens = 4;
  // Stream duration, given when the stream ends
  optional uint32 stream_duration = 5;
  // An error message, if an error occurs
  optional string error_message = 6;
  // The test record ID, given when the stream ends
  optional string prompt_test_record_id = 7;
}
