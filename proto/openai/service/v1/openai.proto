syntax = "proto3";

// buf:lint:ignore PACKAGE_DIRECTORY_MATCH
package openai.service.v1;

// Type of OpenAI Model
enum OpenAIModel {
  // OpenAI Model is not Specified
  OPEN_AI_MODEL_UNSPECIFIED = 0;
  // OpenAI GPT3.5 Turbo 4K
  OPEN_AI_MODEL_GPT3_5_TURBO_4K = 1;
  // OpenAI GPT3.5 Turbo 16K
  OPEN_AI_MODEL_GPT3_5_TURBO_16K = 2;
  // OpenAI GPT4 8K
  OPEN_AI_MODEL_GPT4_8K = 3;
  // OpenAI GPT4 32K
  OPEN_AI_MODEL_GPT4_32K = 4;
}

// Type of OpenAI Message
enum OpenAIMessage {
  // OpenAI Message type is not Specified
  OPEN_AI_MESSAGE_UNSPECIFIED = 0;
  // OpenAI System message
  OPEN_AI_MESSAGE_SYSTEM = 1;
  // OpenAI User message
  OPEN_AI_MESSAGE_USER = 2;
  // OpenAI Assistant message
  OPEN_AI_MESSAGE_ASSISTANT = 3;
  // OpenAI Function message
  OPEN_AI_MESSAGE_FUNCTION = 4;
}

// The OpenAIService service definition.
service OpenAIService {
  // Request a regular LLM prompt
  rpc OpenAIPrompt(OpenAIPromptRequest) returns (PromptResponse) {}
  // Request a streaming LLM prompt
  rpc OpenAIStream(OpenAIPromptRequest) returns (stream PromptResponse) {}
}

// A Request for an OpenAI regular LLM Prompt
message OpenAIPromptRequest {
  // OpenAI Model identifier
  OpenAIModel model = 1;
  // Prompt Content
  repeated OpenAIMessage content = 2 [packed = true];
  // Temperature Sampling: https://platform.openai.com/docs/api-reference/chat/create#temperature
  optional float temperature = 3;
  // Nucleus Sampling: https://platform.openai.com/docs/api-reference/chat/create#top_p
  optional float top_p = 4;
  // Maximum Tokens after which the prompt will stop generating a response;
  optional uint32 max_tokens = 5;
  // Unique user ID to keep track of conversations;
  optional string user_id = 6;
  // Penalize New tokens: https://platform.openai.com/docs/api-reference/chat/create#presence_penalty
  optional float presence_penalty = 7;
  // Penalize Repeated tokens: https://platform.openai.com/docs/api-reference/chat/create#frequency_penalty
  optional float frequency_penalty = 8;
}

// An OpenAI Prompt Response Message
message PromptResponse {
  // Prompt Content
  string content = 1;
  // Number of tokens used for the prompt
  uint32 prompt_tokens = 2;
  // Number of tokens used to generate the completion
  uint32 completion_tokens = 3;
  // Total number of tokens used to generate the response
  uint32 total_tokens = 4;
}

// An OpenAI Streaming Response Message
message StreamResponse {
  // Prompt Content
  string content = 1;
  // Finish reason, if this is the last message
  optional string finish_reason = 2;
}
