syntax = "proto3";

package openai.v1;

option go_package = "github.com/basemind-ai/monorepo/gen/openaiconnector";

// Type of OpenAI Model
enum OpenAIModel {
  // OpenAI Model is not Specified
  OPEN_AI_MODEL_UNSPECIFIED = 0;
  // OpenAI GPT3.5 Turbo 4K
  OPEN_AI_MODEL_GPT3_5_TURBO_4K = 1;
  // OpenAI GPT3.5 Turbo 16K
  OPEN_AI_MODEL_GPT3_5_TURBO_16K = 2;
  // OpenAI GPT4 8K
  OPEN_AI_MODEL_GPT4_8K = 3;
  // OpenAI GPT4 32K
  OPEN_AI_MODEL_GPT4_32K = 4;
}

// Type of OpenAI Message
enum OpenAIMessageRole {
  // OpenAI Message type is not Specified
  OPEN_AI_MESSAGE_ROLE_UNSPECIFIED = 0;
  // OpenAI System message
  OPEN_AI_MESSAGE_ROLE_SYSTEM = 1;
  // OpenAI User message
  OPEN_AI_MESSAGE_ROLE_USER = 2;
  // OpenAI Assistant message
  OPEN_AI_MESSAGE_ROLE_ASSISTANT = 3;
  // OpenAI Function message
  OPEN_AI_MESSAGE_ROLE_FUNCTION = 4;
}

// The OpenAIService service definition.
service OpenAIService {
  // Request a regular LLM prompt
  rpc OpenAIPrompt(OpenAIPromptRequest) returns (OpenAIPromptResponse) {}
  // Request a streaming LLM prompt
  rpc OpenAIStream(OpenAIPromptRequest) returns (stream OpenAIStreamResponse) {}
}

// An OpenAI function call
message OpenAIFunctionCall {
  // The signature of the function arguments
  string arguments = 1;
  // The function name
  string name = 2;
}

// An OpenAI Chat Message
message OpenAIMessage {
  // The content of the message
  optional string content = 1;
  // The role of the message
  OpenAIMessageRole role = 2;
  // Name of the message author or function name
  optional string name = 3;
  // The signature function to invoke, if any
  optional OpenAIFunctionCall function_call = 4;
}

// OpenAI API Request parameters
message OpenAIModelParameters {
  // Temperature Sampling: https://platform.openai.com/docs/api-reference/chat/create#temperature
  optional float temperature = 1;
  // Nucleus Sampling: https://platform.openai.com/docs/api-reference/chat/create#top_p
  optional float top_p = 2;
  // Maximum Tokens after which the prompt will stop generating a response;
  optional uint32 max_tokens = 3;
  // Penalize New tokens: https://platform.openai.com/docs/api-reference/chat/create#presence_penalty
  optional float presence_penalty = 4;
  // Penalize Repeated tokens: https://platform.openai.com/docs/api-reference/chat/create#frequency_penalty
  optional float frequency_penalty = 5;
}

// A Request for an OpenAI regular LLM Prompt
message OpenAIPromptRequest {
  // OpenAI Model identifier
  OpenAIModel model = 1;
  // Prompt Messages
  repeated OpenAIMessage messages = 2;
  // OpenAI API Request parameters
  OpenAIModelParameters parameters = 3;
  // Unique application ID to keep track of conversations;
  optional string application_id = 4;
}

// An OpenAI Prompt Response Message
message OpenAIPromptResponse {
  // Prompt Content
  string content = 1;
  // Count of the request tokens, as returned by the Cohere /tokenize endpoint
  uint32 request_tokens_count = 2;
  // Count of the response tokens, as returned by the Cohere /tokenize endpoint
  uint32 response_tokens_count = 3;
  // Finish reason
  string finish_reason = 4;
}

// An OpenAI Streaming Response Message
message OpenAIStreamResponse {
  // Prompt Content
  string content = 1;
  // Finish reason, if this is the last message
  optional string finish_reason = 2;
  // Count of the request tokens, as returned by the Cohere /tokenize endpoint
  optional uint32 request_tokens_count = 3;
  // Count of the response tokens, as returned by the Cohere /tokenize endpoint
  optional uint32 response_tokens_count = 4;
}
